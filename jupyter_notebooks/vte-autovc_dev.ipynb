{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, pdb, pickle, random\n",
    "       \n",
    "from multiprocessing import Process, Manager   \n",
    "\n",
    "\n",
    "class Utterances(data.Dataset):\n",
    "    \"\"\"Dataset class for the Utterances dataset.\"\"\"\n",
    "\n",
    "    # this object will contain both melspecs and speaker embeddings taken from the train.pkl\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
    "        self.root_dir = config.data_dir\n",
    "        self.len_crop = config.len_crop\n",
    "        self.step = 10\n",
    "        self.file_name = config.file_name\n",
    "        self.one_hot = config.one_hot\n",
    "\n",
    "        # metaname = os.path.join(self.root_dir, \"all_meta_data.pkl\")\n",
    "        meta_all_data = pickle.load(open('./all_meta_data.pkl', \"rb\"))\n",
    "        # split into training data\n",
    "        num_training_speakers=config.train_size\n",
    "        random.seed(1)\n",
    "        training_indices =  random.sample(range(0, len(meta_all_data)), num_training_speakers)\n",
    "        training_set = []\n",
    "\n",
    "        meta_training_speaker_all_uttrs = []\n",
    "        # make list of training speakers\n",
    "        for idx in training_indices:\n",
    "            meta_training_speaker_all_uttrs.append(meta_all_data[idx])\n",
    "        # get training files\n",
    "        for speaker_info in meta_training_speaker_all_uttrs:\n",
    "            speaker_id_emb = speaker_info[:2]\n",
    "            speaker_uttrs = speaker_info[2:]\n",
    "            num_files = len(speaker_uttrs) # first 2 entries are speaker ID and speaker_emb)\n",
    "            training_file_num = round(num_files*0.9)\n",
    "            training_file_indices = random.sample(range(0, num_files), training_file_num)\n",
    "\n",
    "            training_file_names = []\n",
    "            for index in training_file_indices:\n",
    "                fileName = speaker_uttrs[index]\n",
    "                training_file_names.append(fileName)\n",
    "            training_set.append(speaker_id_emb+training_file_names)\n",
    "            # training_file_names_array = np.asarray(training_file_names)\n",
    "            # training_file_indices_array = np.asarray(training_file_indices)\n",
    "            # test_file_indices = np.setdiff1d(np.arange(num_files_in_subdir), training_file_indices_array)\n",
    "        meta = training_set\n",
    "        # pdb.set_trace()\n",
    "        with open('././my_data/my_autovc/model_data/' +self.file_name +'/training_meta_data.pkl', 'wb') as train_pack:\n",
    "            pickle.dump(training_set, train_pack)\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        training_info = pickle.load(open('././my_data/my_autovc/model_data/' +self.file_name +'/training_meta_data.pkl', 'rb'))\n",
    "        num_speakers_seq = np.arange(len(training_info))\n",
    "        self.one_hot_array = np.eye(len(training_info))[num_speakers_seq]\n",
    "        self.spkr_id_list = [spkr[0] for spkr in training_info]\n",
    "\n",
    "        \"\"\"Load data using multiprocessing\"\"\"\n",
    "        manager = Manager()\n",
    "        meta = manager.list(meta)\n",
    "        dataset = manager.list(len(meta)*[None])  \n",
    "        processes = []\n",
    "        # uses a different process thread for every self.steps of the meta content\n",
    "        for i in range(0, len(meta), self.step):\n",
    "            p = Process(target=self.load_data, \n",
    "                        args=(meta[i:i+self.step],dataset,i))  \n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        # pdb.set_trace()    \n",
    "        self.train_dataset = list(dataset)\n",
    "        self.num_tokens = len(self.train_dataset)\n",
    "        \n",
    "        print('Finished loading the dataset...')\n",
    "        \n",
    "    # this function is called within the class init (after self.data_loader its the arguments) \n",
    "    def load_data(self, submeta, dataset, idx_offset):  \n",
    "        for k, sbmt in enumerate(submeta):    \n",
    "            uttrs = len(sbmt)*[None]\n",
    "            # pdb.set_trace()\n",
    "            for j, tmp in enumerate(sbmt):\n",
    "                if j < 2:  # fill in speaker id and embedding\n",
    "                    uttrs[j] = tmp\n",
    "                else: # load the mel-spectrograms\n",
    "                    uttrs[j] = np.load(os.path.join(self.root_dir, tmp))\n",
    "            dataset[idx_offset+k] = uttrs\n",
    "                   \n",
    "    \"\"\"__getitem__ selects a speaker and chooses a random subset of data (in this case\n",
    "    an utterance) and randomly crops that data. It also selects the corresponding speaker\n",
    "    embedding and loads that up. It will now also get corresponding pitch contour for such a file\"\"\" \n",
    "    def __getitem__(self, index):\n",
    "        # pick a random speaker\n",
    "        dataset = self.train_dataset \n",
    "        # list_uttrs is literally a list of utterance from a single speaker\n",
    "        list_uttrs = dataset[index]\n",
    "        # pdb.set_trace()\n",
    "        emb_org = list_uttrs[1]\n",
    "        speaker_name = list_uttrs[0]\n",
    "        # pick random uttr with random crop\n",
    "        a = np.random.randint(2, len(list_uttrs))\n",
    "        uttr_info = list_uttrs[a]\n",
    "        \n",
    "        spmel_tmp = uttr_info\n",
    "        #spmel_tmp = uttr_info[0]\n",
    "        #pitch_tmp = uttr_info[1]\n",
    "        if spmel_tmp.shape[0] < self.len_crop:\n",
    "            len_pad = self.len_crop - spmel_tmp.shape[0]\n",
    "            uttr = np.pad(spmel_tmp, ((0,len_pad),(0,0)), 'constant')\n",
    "        #    pitch = np.pad(pitch_tmp, ((0,len_pad),(0,0)), 'constant')\n",
    "        elif spmel_tmp.shape[0] > self.len_crop:\n",
    "            left = np.random.randint(spmel_tmp.shape[0]-self.len_crop)\n",
    "            uttr = spmel_tmp[left:left+self.len_crop, :]\n",
    "        #    pitch = pitch_tmp[left:left+self.len_crop, :]\n",
    "        else:\n",
    "            uttr = spmel_tmp\n",
    "        #    pitch = pitch_tmp    \n",
    "\n",
    "        # find out where speaker is in the order of the training list for one-hot\n",
    "        for i, spkr_id in enumerate(self.spkr_id_list):\n",
    "            if speaker_name == spkr_id:\n",
    "                spkr_label = i\n",
    "                break\n",
    "        one_hot_spkr_label = self.one_hot_array[spkr_label]\n",
    "        if self.one_hot==False:\n",
    "            return uttr, emb_org, speaker_name # pitch\n",
    "        else:\n",
    "            return uttr, one_hot_spkr_label, speaker_name\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of spkrs.\"\"\"\n",
    "        return self.num_tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def get_loader(config, num_workers=0):\n",
    "    \"\"\"Build and return a data loader.\"\"\"\n",
    "    \n",
    "    dataset = Utterances(config)\n",
    "    \n",
    "    worker_init_fn = lambda x: np.random.seed((torch.initial_seed()) % (2**32))\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                  batch_size=config.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=num_workers,\n",
    "                                  drop_last=True,\n",
    "                                  worker_init_fn=worker_init_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pdb\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearNorm(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.linear_layer.weight,\n",
    "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "\n",
    "class ConvNorm(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
    "        super(ConvNorm, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        conv_signal = self.conv(signal)\n",
    "        return conv_signal\n",
    "\n",
    "# \"4.2. The Content Encoder\"\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder module:\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_neck, dim_emb, freq):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dim_neck = dim_neck\n",
    "        self.freq = freq\n",
    "        convolutions = []\n",
    "        for i in range(3):\n",
    "        # \"the input to the content encoder is the 80-dimensional mel-spectrogram of X1 concatenated with the speaker embedding\" - I think the embeddings are copy pasted from a dataset, as the Speaker Decoder is pretrained and may not actually appear in this implementation?\n",
    "            conv_layer = nn.Sequential(\n",
    "        # \"the input to the content encoder is the 80-dimensional mel-spectrogram of X1 concatenated with the speaker embedding. The concatenated features are fed into three 5 × 1 convolutional layers, each followed by batch normalization and ReLU activation. The number of channels is 512\"\n",
    "                ConvNorm(80+dim_emb if i==0 else 512,\n",
    "                         512,\n",
    "                         kernel_size=5, stride=1,\n",
    "                         padding=2,\n",
    "                         dilation=1, w_init_gain='relu'),\n",
    "                nn.BatchNorm1d(512))\n",
    "            convolutions.append(conv_layer)\n",
    "        self.convolutions = nn.ModuleList(convolutions)\n",
    "        \n",
    "        # \"Both the forward and backward cell dimensions are 32, so their (LSTMs) combined dimension is 64.\"\n",
    "        self.lstm = nn.LSTM(512, dim_neck, 2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # c_org is speaker embedding\n",
    "    def forward(self, x, c_org):\n",
    "        x = x.squeeze(1).transpose(2,1)\n",
    "        # broadcasts c_org to a compatible shape to merge with x\n",
    "        c_org = c_org.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
    "        x = torch.cat((x, c_org), dim=1)\n",
    "        for conv in self.convolutions:\n",
    "            x = F.relu(conv(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        self.lstm.flatten_parameters()\n",
    "        # lstms output 64 dim\n",
    "        outputs, _ = self.lstm(x)\n",
    "        # backward is the first half of dimensions, forward is the second half\n",
    "        # pdb.set_trace()\n",
    "        out_forward = outputs[:, :, :self.dim_neck]\n",
    "        out_backward = outputs[:, :, self.dim_neck:]\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        codes = []\n",
    "        \n",
    "        # for each timestep, skipping self.freq frames\n",
    "        for i in range(0, outputs.size(1), self.freq):\n",
    "            # remeber that i is self.freq, not increments of 1)\n",
    "            codes.append(torch.cat((out_forward[:,i+self.freq-1,:],out_backward[:,i,:]), dim=-1))\n",
    "        \n",
    "        # if self.freq is 32, then codes is a list of 4 tensors of size 64\n",
    "        return codes\n",
    "      \n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder module:\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_neck, dim_emb, dim_pre):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(dim_neck*2+dim_emb, dim_pre, 1, batch_first=True)\n",
    "        \n",
    "        convolutions = []\n",
    "        for i in range(3):\n",
    "            conv_layer = nn.Sequential(\n",
    "                ConvNorm(dim_pre,\n",
    "                         dim_pre,\n",
    "                         kernel_size=5, stride=1,\n",
    "                         padding=2,\n",
    "                         dilation=1, w_init_gain='relu'),\n",
    "                nn.BatchNorm1d(dim_pre))\n",
    "            convolutions.append(conv_layer)\n",
    "        self.convolutions = nn.ModuleList(convolutions)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(dim_pre, 1024, 2, batch_first=True)\n",
    "        self.linear_projection = LinearNorm(1024, 80)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #self.lstm1.flatten_parameters()\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        for conv in self.convolutions:\n",
    "            x = F.relu(conv(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        outputs, _ = self.lstm2(x)\n",
    "        \n",
    "        decoder_output = self.linear_projection(outputs)\n",
    "\n",
    "        return decoder_output   \n",
    "    \n",
    "# Still part of Decoder as indicated in paper Fig. 3 (c) - last two blocks \n",
    "class Postnet(nn.Module):\n",
    "    \"\"\"Postnet\n",
    "        - Five 1-d convolution with 512 channels and kernel size 5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Postnet, self).__init__()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(80, 512,\n",
    "                         kernel_size=5, stride=1,\n",
    "                         padding=2,\n",
    "                         dilation=1, w_init_gain='tanh'),\n",
    "                nn.BatchNorm1d(512))\n",
    "        )\n",
    "\n",
    "        for i in range(1, 5 - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    ConvNorm(512,\n",
    "                             512,\n",
    "                             kernel_size=5, stride=1,\n",
    "                             padding=2,\n",
    "                             dilation=1, w_init_gain='tanh'),\n",
    "                    nn.BatchNorm1d(512))\n",
    "            )\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(512, 80,\n",
    "                         kernel_size=5, stride=1,\n",
    "                         padding=2,\n",
    "                         dilation=1, w_init_gain='linear'),\n",
    "                nn.BatchNorm1d(80))\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.convolutions) - 1):\n",
    "            x = torch.tanh(self.convolutions[i](x))\n",
    "\n",
    "        x = self.convolutions[-1](x)\n",
    "\n",
    "        return x    \n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network.\"\"\"\n",
    "    def __init__(self, dim_neck, dim_emb, dim_pre, freq):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(dim_neck, dim_emb, freq)\n",
    "        self.decoder = Decoder(dim_neck, dim_emb, dim_pre)\n",
    "        self.postnet = Postnet()\n",
    "\n",
    "    def forward(self, x, c_org, c_trg):\n",
    "\n",
    "        # codes is a LIST of tensors                \n",
    "        codes = self.encoder(x, c_org)\n",
    "        # if no c_trg given, then just return the formatted encoder codes\n",
    "        if c_trg is None:\n",
    "            # concatenates the by stacking over the last (in 2D this would be vertical) dimensio by stacking over the last (in 2D this would be vertical) dimension. For lists it means the same\n",
    "            return torch.cat(codes, dim=-1)\n",
    "\n",
    "        # list of reformatted codes        \n",
    "        tmp = []\n",
    "        for code in codes:\n",
    "            # reformatting tmp from list to tensor, and giving it new dim of 128 (x.size(1))\n",
    "            tmp.append(code.unsqueeze(1).expand(-1,int(x.size(1)/len(codes)),-1))\n",
    "        code_exp = torch.cat(tmp, dim=1)\n",
    "        \n",
    "        # concat reformated encoder output with target speaker embedding\n",
    "        encoder_outputs = torch.cat((code_exp, c_trg.unsqueeze(1).expand(-1,x.size(1),-1)), dim=-1)\n",
    "        mel_outputs = self.decoder(encoder_outputs)\n",
    "        # then put mel_ouputs through remaining postnet section of NN\n",
    "        # the postnet process produces the RESIDUAL information that gets added to the mel output\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs.transpose(2,1))\n",
    "        #pdb.set_trace() \n",
    "        # add together, as done in Fig. 3 (c) ensuring the mel_out_psnt is same shape (2,128,80). new mel_out_psnt will be the same\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet.transpose(2,1)\n",
    "       \n",
    "        #insert channel dimension into tensors to become (2,1,128,80)\n",
    "        mel_outputs = mel_outputs.unsqueeze(1)\n",
    "        mel_outputs_postnet = mel_outputs_postnet.unsqueeze(1)\n",
    "        \n",
    "        return mel_outputs, mel_outputs_postnet, torch.cat(codes, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import utils\n",
    "from scipy.signal import medfilt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import time, pdb\n",
    "import datetime\n",
    "\n",
    "# SOLVER IS THE MAIN SETUP FOR THE NN ARCHITECTURE. INSIDE SOLVER IS THE GENERATOR (G)\n",
    "class Solver(object):\n",
    "\n",
    "    def __init__(self, vcc_loader, config):\n",
    "        \"\"\"Initialize configurations.\"\"\"\n",
    "    \n",
    "        \n",
    "        # Data loader.\n",
    "        self.vcc_loader = vcc_loader\n",
    "\n",
    "        # Model configurations.\n",
    "        self.lambda_cd = config.lambda_cd\n",
    "        self.dim_neck = config.dim_neck\n",
    "        self.dim_emb = config.dim_emb\n",
    "        self.dim_pre = config.dim_pre\n",
    "        self.freq = config.freq\n",
    "        self.shape_adapt = config.shape_adapt\n",
    "        self.which_cuda = config.which_cuda\n",
    "\n",
    "        # Training configurations.\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_iters = config.num_iters\n",
    "        self.load_ckpts = config.load_ckpts\n",
    "        self.file_name = config.file_name\n",
    "        self.one_hot = config.one_hot\n",
    "        self.psnt_loss_weight = config.psnt_loss_weight \n",
    "        self.prnt_loss_weight = config.prnt_loss_weight \n",
    "        self.adam_init = config.adam_init\n",
    "\n",
    "\n",
    "        # Miscellaneous.\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(f'cuda:{self.which_cuda}' if self.use_cuda else 'cpu')\n",
    "        self.log_step = config.log_step\n",
    "        self.shape_adapt = config.shape_adapt\n",
    "        self.ckpt_freq = config.ckpt_freq\n",
    "        self.spec_freq = config.spec_freq\n",
    "\n",
    "         # Build the model and tensorboard.\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        self.G = Generator(self.dim_neck, self.dim_emb, self.dim_pre, self.freq)        \n",
    "        \n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.adam_init)\n",
    "        tester=1\n",
    "        if self.load_ckpts!='':\n",
    "            g_checkpoint = torch.load(self.load_ckpts)\n",
    "            self.G.load_state_dict(g_checkpoint['model_state_dict'])\n",
    "            self.g_optimizer.load_state_dict(g_checkpoint['optimizer_state_dict'])\n",
    "            # fixes tensors on different devices error\n",
    "            # https://github.com/pytorch/pytorch/issues/2830\n",
    "            for state in self.g_optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        state[k] = v.cuda()\n",
    "\n",
    "            self.previous_ckpt_iters = g_checkpoint['iteration']\n",
    "            tester=2\n",
    "        else:\n",
    "            self.previous_ckpt_iters = 0\n",
    "        self.G.to(self.device)\n",
    "\n",
    "    def reset_grad(self):\n",
    "        \"\"\"Reset the gradient buffers.\"\"\"\n",
    "        self.g_optimizer.zero_grad()\n",
    "      \n",
    "    \n",
    "    #=====================================================================================================================================#\n",
    "   \n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        # Set data loader.\n",
    "        data_loader = self.vcc_loader\n",
    "        hist_arr = np.array([0,0,0])\n",
    "        # Print logs in specified order\n",
    "        keys = ['G/loss_id','G/loss_id_psnt','G/loss_cd']\n",
    "            \n",
    "        # Start training.\n",
    "        print('Start training...')\n",
    "        start_time = time.time()\n",
    "        for i in range(self.previous_ckpt_iters, self.num_iters):\n",
    "\n",
    "            # =================================================================================== #\n",
    "            #                             1. Preprocess input data                                #\n",
    "            # =================================================================================== #\n",
    "\n",
    "            # Fetch data.\n",
    "            # THE NEXT(DATA_ITER) FUNCTION USES THE DATALOADERS __GETITEM__ FUNCTION, SEEMINGLY\n",
    "            # ITER AND NEXT FUNCTIONS WORK TOGETHER TO PRODUCE A COLLATED BATCH OF EXAMPLES \n",
    "            try:\n",
    "                x_real, emb_org, speaker_name = next(data_iter)\n",
    "            except:\n",
    "                data_iter = iter(data_loader)\n",
    "                x_real, emb_org, speaker_name = next(data_iter)\n",
    "            \n",
    "            \n",
    "        \n",
    "            x_real = x_real.to(self.device) \n",
    "            emb_org = emb_org.to(self.device).float() \n",
    "                        \n",
    "       \n",
    "            # =================================================================================== #\n",
    "            #                               2. Train the generator                                #\n",
    "            # =================================================================================== #\n",
    "            # informs generator to be in train mode \n",
    "            self.G = self.G.train()\n",
    "                        \n",
    "            # Identity mapping loss\n",
    "            # x_identic_psnt consists of the original mel + the residual definiton added ontop\n",
    "            x_identic, x_identic_psnt, code_real = self.G(x_real, emb_org, emb_org)\n",
    "            # SHAPES OF X_REAL AND X_INDETIC/PSNT ARE NOT THE SAME AND MAY GIVE INCORRECT LOSS VALUES\n",
    "            residual_from_psnt = x_identic_psnt - x_identic\n",
    "            # pdb.set_trace()\n",
    "            if self.shape_adapt == True:\n",
    "                x_identic = x_identic.squeeze(1)\n",
    "                x_identic_psnt = x_identic_psnt.squeeze(1)\n",
    "                residual_from_psnt = residual_from_psnt.squeeze(1)\n",
    "            g_loss_id = F.l1_loss(x_real, x_identic)   \n",
    "            g_loss_id_psnt = F.l1_loss(x_real, x_identic_psnt)   \n",
    "            \n",
    "            # Code semantic loss. For calculating this, there is no target embedding\n",
    "            code_reconst = self.G(x_identic_psnt, emb_org, None)\n",
    "            # gets the l1 loss between original encoder output and reconstructed encoder output\n",
    "            g_loss_cd = F.l1_loss(code_real, code_reconst)\n",
    "\n",
    "\n",
    "            # Backward and optimize.\n",
    "            # interesting - the loss is a sum of the decoder loss and the melspec loss\n",
    "            g_loss = (self.prnt_loss_weight * g_loss_id) + (self.psnt_loss_weight * g_loss_id_psnt) + (self.lambda_cd * g_loss_cd)\n",
    "            self.reset_grad()\n",
    "            g_loss.backward()\n",
    "            #pdb.set_trace()\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            # Logging.\n",
    "            loss = {}\n",
    "            loss['G/loss_id'] = g_loss_id.item()\n",
    "            loss['G/loss_id_psnt'] = g_loss_id_psnt.item()\n",
    "            loss['G/loss_cd'] = g_loss_cd.item()\n",
    "            \n",
    "            if i==0:\n",
    "                hist_arr = np.array([g_loss_id.item(), g_loss_id_psnt.item(), g_loss_cd.item()])\n",
    "            else:\n",
    "                temp_arr = np.array([g_loss_id.item(), g_loss_id_psnt.item(), g_loss_cd.item()])\n",
    "                hist_arr = np.vstack((hist_arr, temp_arr))\n",
    "            # =================================================================================== #\n",
    "            #                                 4. Miscellaneous                                    #\n",
    "            # =================================================================================== #\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            # Print out training information.\n",
    "            if (i+1) % self.log_step == 0:\n",
    "                et = time.time() - start_time\n",
    "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
    "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
    "                for tag in keys:\n",
    "                    log += \", {}: {:.4f}\".format(tag, loss[tag])\n",
    "                print(log)\n",
    "\n",
    "            if (i+1) % self.spec_freq == 0:\n",
    "                # save x and x_hat images\n",
    "                x_real = x_real.cpu().data.numpy()\n",
    "                if self.shape_adapt == True:\n",
    "                    x_identic = x_identic.cpu().data.numpy()\n",
    "                    x_identic_psnt = x_identic_psnt.cpu().data.numpy()\n",
    "                    residual_from_psnt = residual_from_psnt.cpu().data.numpy()\n",
    "                else:\n",
    "                    x_identic = x_identic.squeeze(1).cpu().data.numpy()\n",
    "                    x_identic_psnt = x_identic_psnt.squeeze(1).cpu().data.numpy()\n",
    "                    residual_from_psnt = residual_from_psnt.squeeze(1).cpu().data.numpy()\n",
    "                specs_list = []\n",
    "                for arr in x_real:\n",
    "                    specs_list.append(arr)\n",
    "                for arr in x_identic:\n",
    "                    specs_list.append(arr)\n",
    "                for arr in residual_from_psnt:\n",
    "                    specs_list.append(arr)\n",
    "                for arr in x_identic_psnt:\n",
    "                    specs_list.append(arr)\n",
    "                columns = 2\n",
    "                rows = 4\n",
    "                fig, axs = plt.subplots(4,2)\n",
    "                fig.tight_layout()\n",
    "                for j in range(0, columns*rows):\n",
    "                    spec = np.rot90(specs_list[j])\n",
    "                    fig.add_subplot(rows, columns, j+1)\n",
    "                    if j == 5 or j == 6:\n",
    "                        #pdb.set_trace()\n",
    "                        spec = spec - np.min(spec)\n",
    "                        plt.clim(0,1)\n",
    "                    plt.imshow(spec)\n",
    "                    name = speaker_name[j%2]\n",
    "                    plt.title(name)\n",
    "                    plt.colorbar()\n",
    "                plt.savefig('././my_data/my_autovc/model_data/' +self.file_name +'/image_comparison/' +str(i+1) +'iterations')\n",
    "                plt.close(name)\n",
    "                # save_recon_image(x_real, x_identic_psnt, speaker_name)    \n",
    "                \n",
    "            if (i+1) % self.ckpt_freq == 0:\n",
    "                print('Saving model...')\n",
    "                checkpoint = {'model_state_dict' : self.G.state_dict(),\n",
    "                    'optimizer_state_dict': self.g_optimizer.state_dict(),\n",
    "                    'iteration': i+1,\n",
    "                    'loss': loss}\n",
    "                torch.save(checkpoint, '././my_data/my_autovc/model_data/' +self.file_name +'/ckpts/' +'ckpt_' +str(i+1) +'.pth.tar')\n",
    "                # plotting history since last checkpoint downsampled by 100\n",
    "                print('Saving loss visuals...')\n",
    "                num_cols=1\n",
    "                num_graph_vals = 200\n",
    "                down_samp_size = math.ceil(self.ckpt_freq/num_graph_vals)\n",
    "                modified_array = hist_arr[-self.ckpt_freq::down_samp_size,:]\n",
    "                file_path = '././my_data/my_autovc/model_data/' +self.file_name +'/ckpts/' +'ckpt_' +str(i+1) +'_loss.png'\n",
    "                labels = ['iter_steps','loss','loss_id','loss_id_psnt','loss_cd']\n",
    "                utils.saveContourPlots(modified_array, file_path, labels, num_cols) \n",
    "                if (i+1) % (self.ckpt_freq*2) == 0:\n",
    "                    print('saving loss visuals of all history...')\n",
    "                    down_samp_size = math.ceil(i/num_graph_vals)\n",
    "                    modified_array = hist_arr[::down_samp_size,:]\n",
    "                    file_path = '././my_data/my_autovc/model_data/' +self.file_name +'/ckpts/' +'ckpt_' +str(i+1) +'_loss_all_history.png'\n",
    "                    utils.saveContourPlots(modified_array, file_path, labels, num_cols) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/homes/bdoc3/my_data/autovc_data/my_autovc/./my_data/my_autovc/model_data/1Hot16FreqL1Loss/config.pkl'\n",
    "config = pickle.load(open(config_path, 'rb'))\n",
    "config.data_dir='/homes/bdoc3/my_data/autovc_data/spmel'\n",
    "config.file_name='testAutoVcWithoutPitchDataUsing1Hot16FreqConfigFile'\n",
    "config.num_iters = 100\n",
    "config.log_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adam_init=0.0001, batch_size=2, ckpt_freq=50000, data_dir='/homes/bdoc3/my_data/autovc_data/spmel', dim_emb=20, dim_neck=32, dim_pre=512, file_name='testAutoVcWithoutPitchDataUsing1Hot16FreqConfigFile', freq=16, lambda_cd=1, len_crop=128, load_ckpts='', log_step=10, num_iters=100, one_hot=True, prnt_loss_weight=1.0, psnt_loss_weight=1.0, shape_adapt=True, spec_freq=10000, train_size=20, which_cuda=1)\n",
      "Finished loading the dataset...\n",
      "Start training...\n",
      "Elapsed [0:00:03], Iteration [10/100], G/loss_id: 0.2987, G/loss_id_psnt: 0.8301, G/loss_cd: 0.1072\n",
      "Elapsed [0:00:06], Iteration [20/100], G/loss_id: 0.1679, G/loss_id_psnt: 0.7784, G/loss_cd: 0.0886\n",
      "Elapsed [0:00:10], Iteration [30/100], G/loss_id: 0.1658, G/loss_id_psnt: 0.7672, G/loss_cd: 0.0836\n",
      "Elapsed [0:00:13], Iteration [40/100], G/loss_id: 0.1555, G/loss_id_psnt: 0.7418, G/loss_cd: 0.0759\n",
      "Elapsed [0:00:16], Iteration [50/100], G/loss_id: 0.1478, G/loss_id_psnt: 0.7397, G/loss_cd: 0.0772\n",
      "Elapsed [0:00:20], Iteration [60/100], G/loss_id: 0.1603, G/loss_id_psnt: 0.7081, G/loss_cd: 0.0684\n",
      "Elapsed [0:00:23], Iteration [70/100], G/loss_id: 0.1485, G/loss_id_psnt: 0.6755, G/loss_cd: 0.0677\n",
      "Elapsed [0:00:26], Iteration [80/100], G/loss_id: 0.1497, G/loss_id_psnt: 0.6527, G/loss_cd: 0.0593\n",
      "Elapsed [0:00:29], Iteration [90/100], G/loss_id: 0.1528, G/loss_id_psnt: 0.6955, G/loss_cd: 0.0613\n",
      "Elapsed [0:00:33], Iteration [100/100], G/loss_id: 0.1324, G/loss_id_psnt: 0.5382, G/loss_cd: 0.0615\n"
     ]
    }
   ],
   "source": [
    "import os, pdb, pickle, argparse, shutil\n",
    "from torch.backends import cudnn\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "def overwrite_dir(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    os.makedirs(directory)\n",
    "        \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     # Model configuration.\n",
    "#     parser.add_argument('--lambda_cd', type=float, default=1, help='weight for hidden code loss')\n",
    "#     parser.add_argument('--dim_neck', type=int, default=32)\n",
    "#     parser.add_argument('--dim_emb', type=int, default=256)\n",
    "#     parser.add_argument('--dim_pre', type=int, default=512)\n",
    "#     parser.add_argument('--freq', type=int, default=32)\n",
    "#     parser.add_argument('--one_hot', type=str2bool, default=False, help='Toggle 1-hot mode')\n",
    "#     parser.add_argument('--shape_adapt', type=str2bool, default=True, help='adjust shapes of tensors to match automatically')\n",
    "#     parser.add_argument('--which_cuda', type=int, default=0, help='Determine which cuda to use')\n",
    "    \n",
    "#     # Training configuration.\n",
    "#     parser.add_argument('--file_name', type=str, default='defaultSetup')\n",
    "#     parser.add_argument('--data_dir', type=str, default='./spmel')\n",
    "#     parser.add_argument('--batch_size', type=int, default=2, help='mini-batch size')\n",
    "#     parser.add_argument('--num_iters', type=int, default=1000000, help='number of total iterations')\n",
    "#     parser.add_argument('--adam_init', type=float, default=0.0001, help='Define initial Adam optimizer learning rate')\n",
    "#     parser.add_argument('--train_size', type=int, default=20, help='Define how many speakers are used in the training set')\n",
    "#     parser.add_argument('--len_crop', type=int, default=128, help='dataloader output sequence length')\n",
    "#     parser.add_argument('--psnt_loss_weight', type=float, default=1.0, help='Determine weight applied to postnet reconstruction loss')\n",
    "#     parser.add_argument('--prnt_loss_weight', type=float, default=1.0, help='Determine weight applied to pre-net reconstruction loss')\n",
    " \n",
    "#     # Miscellaneous.\n",
    "#     parser.add_argument('--load_ckpts', type=str, default='', help='toggle checkpoint load function')\n",
    "#     parser.add_argument('--ckpt_freq', type=int, default=50000, help='frequency in steps to mark checkpoints')\n",
    "#     parser.add_argument('--spec_freq', type=int, default=10000, help='frequency in steps to print reconstruction illustrations')\n",
    "#     parser.add_argument('--log_step', type=int, default=10)\n",
    "#     config = parser.parse_args()\n",
    "\n",
    "if config.one_hot==True:\n",
    "    config.dim_emb=config.train_size\n",
    "\n",
    "print(config)\n",
    "# pdb.set_trace()\n",
    "overwrite_dir('././my_data/my_autovc/model_data/' +config.file_name)\n",
    "os.makedirs('././my_data/my_autovc/model_data/' +config.file_name +'/ckpts')\n",
    "os.makedirs('././my_data/my_autovc/model_data/' +config.file_name +'/generated_wavs')\n",
    "os.makedirs('././my_data/my_autovc/model_data/' +config.file_name +'/image_comparison')\n",
    "with open('././my_data/my_autovc/model_data/' +config.file_name +'/config.pkl', 'wb') as config_file:\n",
    "    pickle.dump(config, config_file)\n",
    "\n",
    "# For fast training.\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Data loader.\n",
    "vcc_loader = get_loader(config)\n",
    "# pass dataloader and configuration params to Solver NN\n",
    "solver = Solver(vcc_loader, config)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvautovc",
   "language": "python",
   "name": "venvautovc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
